{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pranjalgupta/Documents/domain_adaptation/VisDA17\n"
     ]
    }
   ],
   "source": [
    "cd VisDA17/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation = pd.read_csv('train_validation.csv')\n",
    "train_train = pd.read_csv('train_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = []\n",
    "for i in range(2049):\n",
    "    n_list.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    col_names = list(x.columns)\n",
    "    df_temp = pd.DataFrame(col_names)\n",
    "    df_temp = df_temp.T\n",
    "    x.columns = n_list\n",
    "    df_temp.columns = n_list \n",
    "    df_new = pd.concat([df_temp, x]).reset_index(drop=True)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_validation = preprocess(train_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train.columns = n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>2048</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.072639</td>\n",
       "      <td>0.499112</td>\n",
       "      <td>0.434132</td>\n",
       "      <td>0.083198</td>\n",
       "      <td>0.769916</td>\n",
       "      <td>0.118348</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>0.051234</td>\n",
       "      <td>0.218571</td>\n",
       "      <td>0.595028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653926</td>\n",
       "      <td>0.158317</td>\n",
       "      <td>0.029896</td>\n",
       "      <td>0.095829</td>\n",
       "      <td>0.614174</td>\n",
       "      <td>0.440294</td>\n",
       "      <td>0.171968</td>\n",
       "      <td>0.400972</td>\n",
       "      <td>0.204867</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.17477</td>\n",
       "      <td>0.426307</td>\n",
       "      <td>0.263003</td>\n",
       "      <td>0.089927</td>\n",
       "      <td>0.611682</td>\n",
       "      <td>0.126546</td>\n",
       "      <td>0.479191</td>\n",
       "      <td>0.211345</td>\n",
       "      <td>0.397743</td>\n",
       "      <td>0.260577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465741</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.216472</td>\n",
       "      <td>0.262695</td>\n",
       "      <td>0.45587</td>\n",
       "      <td>0.248653</td>\n",
       "      <td>0.163447</td>\n",
       "      <td>0.26568</td>\n",
       "      <td>0.488878</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.166133</td>\n",
       "      <td>0.435502</td>\n",
       "      <td>0.666165</td>\n",
       "      <td>0.664302</td>\n",
       "      <td>0.968701</td>\n",
       "      <td>0.093478</td>\n",
       "      <td>0.618853</td>\n",
       "      <td>0.252837</td>\n",
       "      <td>0.417761</td>\n",
       "      <td>0.527509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700977</td>\n",
       "      <td>0.25281</td>\n",
       "      <td>0.276713</td>\n",
       "      <td>0.24558</td>\n",
       "      <td>0.530361</td>\n",
       "      <td>0.117131</td>\n",
       "      <td>0.223332</td>\n",
       "      <td>0.217703</td>\n",
       "      <td>0.342711</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.168355</td>\n",
       "      <td>0.082178</td>\n",
       "      <td>0.083082</td>\n",
       "      <td>0.731769</td>\n",
       "      <td>1.40427</td>\n",
       "      <td>0.201808</td>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.203689</td>\n",
       "      <td>0.128185</td>\n",
       "      <td>1.53876</td>\n",
       "      <td>...</td>\n",
       "      <td>1.35747</td>\n",
       "      <td>0.065938</td>\n",
       "      <td>0.11997</td>\n",
       "      <td>0.208527</td>\n",
       "      <td>0.543308</td>\n",
       "      <td>0.144607</td>\n",
       "      <td>0.081495</td>\n",
       "      <td>0.343001</td>\n",
       "      <td>0.411405</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.42632</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>0.27894</td>\n",
       "      <td>0.118287</td>\n",
       "      <td>0.426827</td>\n",
       "      <td>0.124699</td>\n",
       "      <td>0.547373</td>\n",
       "      <td>0.031645</td>\n",
       "      <td>0.725066</td>\n",
       "      <td>0.127075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456946</td>\n",
       "      <td>0.10922</td>\n",
       "      <td>0.232193</td>\n",
       "      <td>0.524799</td>\n",
       "      <td>0.439189</td>\n",
       "      <td>0.380236</td>\n",
       "      <td>0.046122</td>\n",
       "      <td>1.17623</td>\n",
       "      <td>0.273351</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55383</td>\n",
       "      <td>0.424792</td>\n",
       "      <td>0.89977</td>\n",
       "      <td>0.063793</td>\n",
       "      <td>0.209213</td>\n",
       "      <td>1.59399</td>\n",
       "      <td>0.024616</td>\n",
       "      <td>1.11058</td>\n",
       "      <td>0.531962</td>\n",
       "      <td>0.113352</td>\n",
       "      <td>1.02561</td>\n",
       "      <td>...</td>\n",
       "      <td>1.01564</td>\n",
       "      <td>0.168447</td>\n",
       "      <td>0.042441</td>\n",
       "      <td>0.194534</td>\n",
       "      <td>0.094992</td>\n",
       "      <td>0.469219</td>\n",
       "      <td>0.111694</td>\n",
       "      <td>0.914614</td>\n",
       "      <td>1.60226</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55384</td>\n",
       "      <td>0.312371</td>\n",
       "      <td>1.08728</td>\n",
       "      <td>0.677936</td>\n",
       "      <td>0.123091</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.187996</td>\n",
       "      <td>0.033468</td>\n",
       "      <td>0.272372</td>\n",
       "      <td>0.197996</td>\n",
       "      <td>0.154091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516358</td>\n",
       "      <td>0.462732</td>\n",
       "      <td>0.371501</td>\n",
       "      <td>0.058603</td>\n",
       "      <td>0.26151</td>\n",
       "      <td>0.333167</td>\n",
       "      <td>0.399097</td>\n",
       "      <td>0.339613</td>\n",
       "      <td>0.175378</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55385</td>\n",
       "      <td>0.087328</td>\n",
       "      <td>0.020529</td>\n",
       "      <td>0.177264</td>\n",
       "      <td>0.795762</td>\n",
       "      <td>0.565213</td>\n",
       "      <td>0.05932</td>\n",
       "      <td>0.977821</td>\n",
       "      <td>0.200136</td>\n",
       "      <td>0.05818</td>\n",
       "      <td>0.939436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686045</td>\n",
       "      <td>0.399481</td>\n",
       "      <td>0.167271</td>\n",
       "      <td>0.913339</td>\n",
       "      <td>0.458404</td>\n",
       "      <td>0.108615</td>\n",
       "      <td>0.067096</td>\n",
       "      <td>0.105445</td>\n",
       "      <td>0.102883</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55386</td>\n",
       "      <td>0.053928</td>\n",
       "      <td>0.108577</td>\n",
       "      <td>0.192186</td>\n",
       "      <td>0.505346</td>\n",
       "      <td>1.14581</td>\n",
       "      <td>1.00218</td>\n",
       "      <td>0.482339</td>\n",
       "      <td>0.202115</td>\n",
       "      <td>0.069665</td>\n",
       "      <td>0.395421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151997</td>\n",
       "      <td>0.131625</td>\n",
       "      <td>0.086903</td>\n",
       "      <td>0.027217</td>\n",
       "      <td>0.741786</td>\n",
       "      <td>0.045459</td>\n",
       "      <td>0.324497</td>\n",
       "      <td>0.484799</td>\n",
       "      <td>0.383081</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55387</td>\n",
       "      <td>0.350594</td>\n",
       "      <td>0.25036</td>\n",
       "      <td>0.237628</td>\n",
       "      <td>0.340595</td>\n",
       "      <td>0.851712</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.433063</td>\n",
       "      <td>0.067823</td>\n",
       "      <td>0.387363</td>\n",
       "      <td>1.1336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260452</td>\n",
       "      <td>0.195759</td>\n",
       "      <td>0.336667</td>\n",
       "      <td>0.076229</td>\n",
       "      <td>0.521279</td>\n",
       "      <td>0.043984</td>\n",
       "      <td>0.163789</td>\n",
       "      <td>0.785553</td>\n",
       "      <td>0.520794</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55388 rows × 2049 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.072639  0.499112  0.434132  0.083198  0.769916  0.118348  0.017374   \n",
       "1       0.17477  0.426307  0.263003  0.089927  0.611682  0.126546  0.479191   \n",
       "2      0.166133  0.435502  0.666165  0.664302  0.968701  0.093478  0.618853   \n",
       "3      0.168355  0.082178  0.083082  0.731769   1.40427  0.201808  0.720339   \n",
       "4       0.42632  0.016337   0.27894  0.118287  0.426827  0.124699  0.547373   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "55383  0.424792   0.89977  0.063793  0.209213   1.59399  0.024616   1.11058   \n",
       "55384  0.312371   1.08728  0.677936  0.123091  0.436874  0.187996  0.033468   \n",
       "55385  0.087328  0.020529  0.177264  0.795762  0.565213   0.05932  0.977821   \n",
       "55386  0.053928  0.108577  0.192186  0.505346   1.14581   1.00218  0.482339   \n",
       "55387  0.350594   0.25036  0.237628  0.340595  0.851712  0.006274  0.433063   \n",
       "\n",
       "              7         8         9  ...      2039      2040      2041  \\\n",
       "0      0.051234  0.218571  0.595028  ...  0.653926  0.158317  0.029896   \n",
       "1      0.211345  0.397743  0.260577  ...  0.465741  0.160505  0.216472   \n",
       "2      0.252837  0.417761  0.527509  ...  0.700977   0.25281  0.276713   \n",
       "3      0.203689  0.128185   1.53876  ...   1.35747  0.065938   0.11997   \n",
       "4      0.031645  0.725066  0.127075  ...  0.456946   0.10922  0.232193   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "55383  0.531962  0.113352   1.02561  ...   1.01564  0.168447  0.042441   \n",
       "55384  0.272372  0.197996  0.154091  ...  0.516358  0.462732  0.371501   \n",
       "55385  0.200136   0.05818  0.939436  ...  0.686045  0.399481  0.167271   \n",
       "55386  0.202115  0.069665  0.395421  ...  0.151997  0.131625  0.086903   \n",
       "55387  0.067823  0.387363    1.1336  ...  0.260452  0.195759  0.336667   \n",
       "\n",
       "           2042      2043      2044      2045      2046      2047      2048  \n",
       "0      0.095829  0.614174  0.440294  0.171968  0.400972  0.204867  4.000000  \n",
       "1      0.262695   0.45587  0.248653  0.163447   0.26568  0.488878         3  \n",
       "2       0.24558  0.530361  0.117131  0.223332  0.217703  0.342711         2  \n",
       "3      0.208527  0.543308  0.144607  0.081495  0.343001  0.411405        10  \n",
       "4      0.524799  0.439189  0.380236  0.046122   1.17623  0.273351         1  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "55383  0.194534  0.094992  0.469219  0.111694  0.914614   1.60226         3  \n",
       "55384  0.058603   0.26151  0.333167  0.399097  0.339613  0.175378         7  \n",
       "55385  0.913339  0.458404  0.108615  0.067096  0.105445  0.102883         3  \n",
       "55386  0.027217  0.741786  0.045459  0.324497  0.484799  0.383081         6  \n",
       "55387  0.076229  0.521279  0.043984  0.163789  0.785553  0.520794        11  \n",
       "\n",
       "[55388 rows x 2049 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_validation.head()\n",
    "processed_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>2048</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.299771</td>\n",
       "      <td>0.511509</td>\n",
       "      <td>0.828543</td>\n",
       "      <td>0.019764</td>\n",
       "      <td>0.182132</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.122813</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>0.063968</td>\n",
       "      <td>0.273126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204556</td>\n",
       "      <td>0.888747</td>\n",
       "      <td>0.191974</td>\n",
       "      <td>0.782018</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.738916</td>\n",
       "      <td>0.560403</td>\n",
       "      <td>0.038522</td>\n",
       "      <td>0.065525</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.225711</td>\n",
       "      <td>1.158407</td>\n",
       "      <td>0.710707</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.254923</td>\n",
       "      <td>0.107571</td>\n",
       "      <td>0.483804</td>\n",
       "      <td>0.117319</td>\n",
       "      <td>0.050823</td>\n",
       "      <td>0.089911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369407</td>\n",
       "      <td>0.684917</td>\n",
       "      <td>0.112559</td>\n",
       "      <td>0.393860</td>\n",
       "      <td>0.008194</td>\n",
       "      <td>1.242947</td>\n",
       "      <td>0.217126</td>\n",
       "      <td>0.017542</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.745959</td>\n",
       "      <td>0.123832</td>\n",
       "      <td>0.347789</td>\n",
       "      <td>0.920212</td>\n",
       "      <td>0.501937</td>\n",
       "      <td>0.291690</td>\n",
       "      <td>0.069421</td>\n",
       "      <td>1.277493</td>\n",
       "      <td>0.252142</td>\n",
       "      <td>0.281135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687543</td>\n",
       "      <td>0.852605</td>\n",
       "      <td>0.657274</td>\n",
       "      <td>0.939006</td>\n",
       "      <td>0.241401</td>\n",
       "      <td>0.122715</td>\n",
       "      <td>0.048302</td>\n",
       "      <td>0.101571</td>\n",
       "      <td>0.724318</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>0.775114</td>\n",
       "      <td>0.120747</td>\n",
       "      <td>0.332695</td>\n",
       "      <td>0.364243</td>\n",
       "      <td>0.257741</td>\n",
       "      <td>0.051794</td>\n",
       "      <td>0.723782</td>\n",
       "      <td>0.268133</td>\n",
       "      <td>0.188663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>0.824354</td>\n",
       "      <td>1.084597</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>0.181253</td>\n",
       "      <td>0.269883</td>\n",
       "      <td>0.618960</td>\n",
       "      <td>0.758847</td>\n",
       "      <td>0.675816</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.258227</td>\n",
       "      <td>0.538900</td>\n",
       "      <td>0.716632</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.681763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117516</td>\n",
       "      <td>0.109879</td>\n",
       "      <td>0.485876</td>\n",
       "      <td>0.099202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189304</td>\n",
       "      <td>0.067041</td>\n",
       "      <td>0.124631</td>\n",
       "      <td>0.131279</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.218049</td>\n",
       "      <td>0.735380</td>\n",
       "      <td>0.788674</td>\n",
       "      <td>0.226990</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152391</td>\n",
       "      <td>0.841276</td>\n",
       "      <td>0.480788</td>\n",
       "      <td>0.278364</td>\n",
       "      <td>1.076535</td>\n",
       "      <td>0.805781</td>\n",
       "      <td>1.613166</td>\n",
       "      <td>0.971334</td>\n",
       "      <td>1.435103</td>\n",
       "      <td>0.456535</td>\n",
       "      <td>0.378424</td>\n",
       "      <td>...</td>\n",
       "      <td>1.108283</td>\n",
       "      <td>0.295676</td>\n",
       "      <td>0.645437</td>\n",
       "      <td>1.446912</td>\n",
       "      <td>0.089683</td>\n",
       "      <td>0.505884</td>\n",
       "      <td>0.117058</td>\n",
       "      <td>0.074087</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152392</td>\n",
       "      <td>0.209867</td>\n",
       "      <td>0.159488</td>\n",
       "      <td>0.374312</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.308527</td>\n",
       "      <td>0.063426</td>\n",
       "      <td>0.094697</td>\n",
       "      <td>0.008711</td>\n",
       "      <td>0.728858</td>\n",
       "      <td>0.304033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359846</td>\n",
       "      <td>0.025460</td>\n",
       "      <td>0.040214</td>\n",
       "      <td>0.101976</td>\n",
       "      <td>0.069732</td>\n",
       "      <td>0.087505</td>\n",
       "      <td>0.261115</td>\n",
       "      <td>0.954588</td>\n",
       "      <td>0.159519</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152393</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>0.700005</td>\n",
       "      <td>0.539272</td>\n",
       "      <td>0.384460</td>\n",
       "      <td>0.326115</td>\n",
       "      <td>0.320055</td>\n",
       "      <td>0.066655</td>\n",
       "      <td>0.106291</td>\n",
       "      <td>0.127190</td>\n",
       "      <td>0.257473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232789</td>\n",
       "      <td>0.408214</td>\n",
       "      <td>0.644243</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.429063</td>\n",
       "      <td>0.648880</td>\n",
       "      <td>0.910902</td>\n",
       "      <td>0.374764</td>\n",
       "      <td>1.009762</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152394</td>\n",
       "      <td>0.577119</td>\n",
       "      <td>0.493479</td>\n",
       "      <td>0.032507</td>\n",
       "      <td>1.142845</td>\n",
       "      <td>1.882441</td>\n",
       "      <td>0.090198</td>\n",
       "      <td>0.581366</td>\n",
       "      <td>1.176283</td>\n",
       "      <td>0.240536</td>\n",
       "      <td>0.373180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055933</td>\n",
       "      <td>1.444496</td>\n",
       "      <td>0.360320</td>\n",
       "      <td>0.502127</td>\n",
       "      <td>0.165515</td>\n",
       "      <td>0.995883</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.470380</td>\n",
       "      <td>0.319674</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152395</td>\n",
       "      <td>0.578684</td>\n",
       "      <td>0.231088</td>\n",
       "      <td>0.548876</td>\n",
       "      <td>0.455778</td>\n",
       "      <td>0.702505</td>\n",
       "      <td>0.261098</td>\n",
       "      <td>0.267570</td>\n",
       "      <td>1.548787</td>\n",
       "      <td>0.108831</td>\n",
       "      <td>0.502032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435221</td>\n",
       "      <td>0.176275</td>\n",
       "      <td>0.201994</td>\n",
       "      <td>0.799998</td>\n",
       "      <td>0.106076</td>\n",
       "      <td>0.663627</td>\n",
       "      <td>0.074393</td>\n",
       "      <td>0.249925</td>\n",
       "      <td>0.029556</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152396 rows × 2049 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.299771  0.511509  0.828543  0.019764  0.182132  0.007299  0.122813   \n",
       "1       0.225711  1.158407  0.710707  0.001759  0.254923  0.107571  0.483804   \n",
       "2       0.745959  0.123832  0.347789  0.920212  0.501937  0.291690  0.069421   \n",
       "3       0.003392  0.775114  0.120747  0.332695  0.364243  0.257741  0.051794   \n",
       "4       0.258227  0.538900  0.716632  0.005680  0.681763  0.000000  0.117516   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "152391  0.841276  0.480788  0.278364  1.076535  0.805781  1.613166  0.971334   \n",
       "152392  0.209867  0.159488  0.374312  0.001313  0.308527  0.063426  0.094697   \n",
       "152393  0.021243  0.700005  0.539272  0.384460  0.326115  0.320055  0.066655   \n",
       "152394  0.577119  0.493479  0.032507  1.142845  1.882441  0.090198  0.581366   \n",
       "152395  0.578684  0.231088  0.548876  0.455778  0.702505  0.261098  0.267570   \n",
       "\n",
       "               7         8         9  ...      2039      2040      2041  \\\n",
       "0       0.003948  0.063968  0.273126  ...  0.204556  0.888747  0.191974   \n",
       "1       0.117319  0.050823  0.089911  ...  0.369407  0.684917  0.112559   \n",
       "2       1.277493  0.252142  0.281135  ...  0.687543  0.852605  0.657274   \n",
       "3       0.723782  0.268133  0.188663  ...  0.009842  0.824354  1.084597   \n",
       "4       0.109879  0.485876  0.099202  ...  0.189304  0.067041  0.124631   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "152391  1.435103  0.456535  0.378424  ...  1.108283  0.295676  0.645437   \n",
       "152392  0.008711  0.728858  0.304033  ...  0.359846  0.025460  0.040214   \n",
       "152393  0.106291  0.127190  0.257473  ...  0.232789  0.408214  0.644243   \n",
       "152394  1.176283  0.240536  0.373180  ...  0.055933  1.444496  0.360320   \n",
       "152395  1.548787  0.108831  0.502032  ...  0.435221  0.176275  0.201994   \n",
       "\n",
       "            2042      2043      2044      2045      2046      2047  2048  \n",
       "0       0.782018  0.001261  0.738916  0.560403  0.038522  0.065525   7.0  \n",
       "1       0.393860  0.008194  1.242947  0.217126  0.017542  0.009100   7.0  \n",
       "2       0.939006  0.241401  0.122715  0.048302  0.101571  0.724318  11.0  \n",
       "3       0.051092  0.181253  0.269883  0.618960  0.758847  0.675816   6.0  \n",
       "4       0.131279  0.006564  0.218049  0.735380  0.788674  0.226990   8.0  \n",
       "...          ...       ...       ...       ...       ...       ...   ...  \n",
       "152391  1.446912  0.089683  0.505884  0.117058  0.074087  0.034667  10.0  \n",
       "152392  0.101976  0.069732  0.087505  0.261115  0.954588  0.159519   8.0  \n",
       "152393  0.137900  0.429063  0.648880  0.910902  0.374764  1.009762   6.0  \n",
       "152394  0.502127  0.165515  0.995883  0.031252  0.470380  0.319674   2.0  \n",
       "152395  0.799998  0.106076  0.663627  0.074393  0.249925  0.029556   2.0  \n",
       "\n",
       "[152396 rows x 2049 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract n number of images for each class from the dataframes. \n",
    "def find_images(x, n):\n",
    "    for i in range(12):\n",
    "        x_temp = x[x['2048']==i]\n",
    "        x_temp_dropped = x_temp.drop(columns=['2048'])\n",
    "        x_n = x_temp_dropped.iloc[0:n]\n",
    "        x_arr = x_n.to_numpy()\n",
    "        if i==0:\n",
    "            f_arr = x_arr\n",
    "        else:\n",
    "            f_arr = np.concatenate((f_arr, x_arr))\n",
    "    return f_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_features = find_images(processed_train_validation, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 2048)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_features = find_images(train_train, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 2048)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.359744, 0.417904, 0.549616, ..., 0.092457, 0.088131, 0.049311],\n",
       "       [0.466131, 1.453661, 1.075784, ..., 0.369129, 0.447492, 0.121355],\n",
       "       [0.389752, 0.49721 , 0.611394, ..., 0.281623, 0.076602, 0.116203],\n",
       "       ...,\n",
       "       [1.170628, 0.124062, 0.137636, ..., 0.169128, 0.341303, 0.89272 ],\n",
       "       [0.04604 , 0.139518, 0.201461, ..., 0.035926, 0.041849, 0.559603],\n",
       "       [0.      , 0.363832, 0.33586 , ..., 0.228646, 0.046212, 0.46298 ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_features = syn_features.astype('float32')\n",
    "real_features = real_features.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target(n):\n",
    "    a = []\n",
    "    for i in range(12):\n",
    "        for j in range(n):\n",
    "            a.append(i)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = prepare_target(n)\n",
    "syn_target = tf.one_hot(tar, depth=12)\n",
    "real_target = tf.one_hot(tar, depth=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12000, 12), dtype=float32, numpy=\n",
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DANN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = tf.data.Dataset.from_tensor_slices((syn_features, syn_target)).shuffle(200).batch(64)\n",
    "da_dataset = tf.data.Dataset.from_tensor_slices((syn_features, syn_target, real_features, real_target)).shuffle(200).batch(32)\n",
    "test_dataset2 = tf.data.Dataset.from_tensor_slices((real_features, real_target)).shuffle(200).batch(64) #Test Dataset over Target (used for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def gradient_layer(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    def grad(dy):\n",
    "        return lamda*-dy, None\n",
    "    return y, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_layer(x, lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dropout, MaxPool2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "#         self.feature_extractor_layer0 = Conv2D(32, kernel_size=(3,3), activation='relu')\n",
    "#         self.feature_extractor_layer1 = BatchNormalization()\n",
    "#         self.feature_extractor_layer2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "#         self.feature_extractor_layer3 = Conv2D(64, kernel_size=(5, 5), activation='relu')\n",
    "#         self.feature_extractor_layer4 = Dropout(0.5)\n",
    "#         self.feature_extractor_layer5 = BatchNormalization()\n",
    "#         self.feature_extractor_layer6 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.feature_extractor_layer0 = Dense(1024, activation='relu')\n",
    "        self.feature_extractor_layer1 = Dense(256, activation='relu')\n",
    "        \n",
    "        #self.label_predictor_layer0 = Dense(100, activation='relu')\n",
    "        #self.label_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer2 = Dense(12, activation=None)\n",
    "        \n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(2, activation=None)\n",
    "        \n",
    "    def call(self, x, train=False, source_train=True, lamda=1.0):\n",
    "        #Feature Extractor\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        x = self.feature_extractor_layer1(x)\n",
    "        #x = self.feature_extractor_layer2(x)\n",
    "        \n",
    "#         x = self.feature_extractor_layer3(x)\n",
    "#         x = self.feature_extractor_layer4(x, training=train)\n",
    "#         x = self.feature_extractor_layer5(x, training=train)\n",
    "#         x = self.feature_extractor_layer6(x)\n",
    "        \n",
    "        #feature = tf.reshape(x, [-1, 53 * 53 * 64])\n",
    "        feature = x\n",
    "        \n",
    "        #Label Predictor\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "        \n",
    "        #lp_x = self.label_predictor_layer0(feature_slice)\n",
    "        #lp_x = self.label_predictor_layer1(lp_x)\n",
    "        l_logits = self.label_predictor_layer2(feature_slice)\n",
    "        \n",
    "        #Domain Predictor\n",
    "        if source_train is True:\n",
    "            return l_logits\n",
    "        else:\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda)    #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            d_logits = self.domain_predictor_layer2(dp_x)\n",
    "            \n",
    "            return l_logits, d_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def loss_func(input_logits, target_labels):\n",
    "#    loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#    return tf.reduce_mean(loss_function(target_labels, input_logits))\n",
    "def loss_func(input_logits, target_labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "\n",
    "def get_loss(l_logits, labels, d_logits=None, domain=None):\n",
    "    if d_logits is None:\n",
    "        return loss_func(l_logits, labels)\n",
    "    else:\n",
    "        return loss_func(l_logits, labels) + loss_func(d_logits, domain)\n",
    "\n",
    "\n",
    "model_optimizer = tf.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_labels = np.vstack([np.tile([1., 0.], [32, 1]),\n",
    "                           np.tile([0., 1.], [32, 1])])\n",
    "domain_labels = domain_labels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "da_acc = []      # Source Domain Accuracy while DA-training\n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_source(s_images, s_labels, lamda=1.0):\n",
    "    images = s_images\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=True, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output, labels)\n",
    "        epoch_accuracy(output, labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_da(s_images, s_labels, t_images=None, t_labels=None, lamda=1.0):\n",
    "    images = tf.concat([s_images, t_images], 0)\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=False, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output[0], labels, output[1], domain_labels)\n",
    "        epoch_accuracy(output[0], labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(t_images, t_labels):\n",
    "    images = t_images\n",
    "    labels = t_labels\n",
    "    \n",
    "    output = model(images, train=False, source_train=True)\n",
    "    epoch_accuracy(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_mode, epochs):\n",
    "    \n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        p = float(epoch) / epochs\n",
    "        lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "        lamda = lamda.astype('float32')\n",
    "\n",
    "        for batch in dataset:\n",
    "            train_func(*batch, lamda=lamda)\n",
    "        \n",
    "        print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy.result()))\n",
    "        acc_list.append(epoch_accuracy.result())\n",
    "        test()\n",
    "        epoch_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "    '''#Testing Dataset (Target Domain)\n",
    "    for batch in test_dataset:\n",
    "        test_step(*batch)\n",
    "        \n",
    "    print(\"Testing Accuracy : {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "    test_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()'''\n",
    "    \n",
    "    #Target Domain (used for Training)\n",
    "    for batch in test_dataset2:\n",
    "        test_step(*batch)\n",
    "    \n",
    "    print(\"Target Domain Accuracy : {:.3%}\".format(epoch_accuracy.result()))\n",
    "    test2_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 :\t Source Accuracy : 94.025%\n",
      "Target Domain Accuracy : 8.683%\n",
      "Training: Epoch 1 :\t Source Accuracy : 97.367%\n",
      "Target Domain Accuracy : 13.692%\n",
      "Training: Epoch 2 :\t Source Accuracy : 98.717%\n",
      "Target Domain Accuracy : 20.633%\n",
      "Training: Epoch 3 :\t Source Accuracy : 98.983%\n",
      "Target Domain Accuracy : 23.908%\n",
      "Training: Epoch 4 :\t Source Accuracy : 99.108%\n",
      "Target Domain Accuracy : 28.150%\n"
     ]
    }
   ],
   "source": [
    "train('source', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.3491959e-07 9.9900913e-01 5.4283920e-08 5.9102084e-10 3.8859307e-07\n",
      "  3.5910719e-07 3.1766400e-04 3.4435321e-05 1.2311645e-04 1.5078674e-04\n",
      "  3.2111289e-04 4.2848504e-05]], shape=(1, 12), dtype=float32)\n",
      "tf.Tensor([0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(12,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "no = 1199\n",
    "ti = syn_features[no]\n",
    "ti = np.expand_dims(ti, axis=0)\n",
    "predd = tf.nn.softmax(model(ti, train=True, source_train=True, lamda=1.0))\n",
    "print(predd)\n",
    "print(syn_target[no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 :\t Source Accuracy : 99.483%\n",
      "Target Domain Accuracy : 27.858%\n",
      "Training: Epoch 1 :\t Source Accuracy : 99.467%\n",
      "Target Domain Accuracy : 14.183%\n",
      "Training: Epoch 2 :\t Source Accuracy : 99.517%\n",
      "Target Domain Accuracy : 14.758%\n",
      "Training: Epoch 3 :\t Source Accuracy : 99.392%\n",
      "Target Domain Accuracy : 14.125%\n",
      "Training: Epoch 4 :\t Source Accuracy : 99.367%\n",
      "Target Domain Accuracy : 18.317%\n",
      "Training: Epoch 5 :\t Source Accuracy : 99.517%\n",
      "Target Domain Accuracy : 21.042%\n",
      "Training: Epoch 6 :\t Source Accuracy : 99.650%\n",
      "Target Domain Accuracy : 26.075%\n",
      "Training: Epoch 7 :\t Source Accuracy : 99.600%\n",
      "Target Domain Accuracy : 24.342%\n",
      "Training: Epoch 8 :\t Source Accuracy : 99.608%\n",
      "Target Domain Accuracy : 25.825%\n",
      "Training: Epoch 9 :\t Source Accuracy : 99.608%\n",
      "Target Domain Accuracy : 30.792%\n",
      "Training: Epoch 10 :\t Source Accuracy : 99.658%\n",
      "Target Domain Accuracy : 25.325%\n",
      "Training: Epoch 11 :\t Source Accuracy : 99.692%\n",
      "Target Domain Accuracy : 21.875%\n",
      "Training: Epoch 12 :\t Source Accuracy : 99.608%\n",
      "Target Domain Accuracy : 32.242%\n",
      "Training: Epoch 13 :\t Source Accuracy : 99.683%\n",
      "Target Domain Accuracy : 27.358%\n",
      "Training: Epoch 14 :\t Source Accuracy : 99.667%\n",
      "Target Domain Accuracy : 25.675%\n",
      "Training: Epoch 15 :\t Source Accuracy : 99.725%\n",
      "Target Domain Accuracy : 27.325%\n",
      "Training: Epoch 16 :\t Source Accuracy : 99.708%\n",
      "Target Domain Accuracy : 26.958%\n",
      "Training: Epoch 17 :\t Source Accuracy : 99.708%\n",
      "Target Domain Accuracy : 28.583%\n",
      "Training: Epoch 18 :\t Source Accuracy : 99.767%\n",
      "Target Domain Accuracy : 28.083%\n",
      "Training: Epoch 19 :\t Source Accuracy : 99.658%\n",
      "Target Domain Accuracy : 35.292%\n",
      "Training: Epoch 20 :\t Source Accuracy : 99.725%\n",
      "Target Domain Accuracy : 28.317%\n",
      "Training: Epoch 21 :\t Source Accuracy : 99.675%\n",
      "Target Domain Accuracy : 39.033%\n",
      "Training: Epoch 22 :\t Source Accuracy : 99.717%\n",
      "Target Domain Accuracy : 40.242%\n",
      "Training: Epoch 23 :\t Source Accuracy : 99.692%\n",
      "Target Domain Accuracy : 38.225%\n",
      "Training: Epoch 24 :\t Source Accuracy : 99.700%\n",
      "Target Domain Accuracy : 34.092%\n",
      "Training: Epoch 25 :\t Source Accuracy : 99.717%\n",
      "Target Domain Accuracy : 26.342%\n",
      "Training: Epoch 26 :\t Source Accuracy : 99.800%\n",
      "Target Domain Accuracy : 23.908%\n",
      "Training: Epoch 27 :\t Source Accuracy : 99.692%\n",
      "Target Domain Accuracy : 16.217%\n",
      "Training: Epoch 28 :\t Source Accuracy : 99.550%\n",
      "Target Domain Accuracy : 32.158%\n",
      "Training: Epoch 29 :\t Source Accuracy : 99.700%\n",
      "Target Domain Accuracy : 27.800%\n",
      "Training: Epoch 30 :\t Source Accuracy : 99.658%\n",
      "Target Domain Accuracy : 28.117%\n",
      "Training: Epoch 31 :\t Source Accuracy : 99.717%\n",
      "Target Domain Accuracy : 22.942%\n",
      "Training: Epoch 32 :\t Source Accuracy : 99.708%\n",
      "Target Domain Accuracy : 20.725%\n",
      "Training: Epoch 33 :\t Source Accuracy : 99.692%\n",
      "Target Domain Accuracy : 14.667%\n",
      "Training: Epoch 34 :\t Source Accuracy : 99.550%\n",
      "Target Domain Accuracy : 23.292%\n",
      "Training: Epoch 35 :\t Source Accuracy : 99.717%\n",
      "Target Domain Accuracy : 17.400%\n",
      "Training: Epoch 36 :\t Source Accuracy : 99.583%\n",
      "Target Domain Accuracy : 22.675%\n",
      "Training: Epoch 37 :\t Source Accuracy : 99.675%\n",
      "Target Domain Accuracy : 22.242%\n",
      "Training: Epoch 38 :\t Source Accuracy : 99.725%\n",
      "Target Domain Accuracy : 19.750%\n",
      "Training: Epoch 39 :\t Source Accuracy : 99.708%\n",
      "Target Domain Accuracy : 20.292%\n",
      "Training: Epoch 40 :\t Source Accuracy : 99.708%\n",
      "Target Domain Accuracy : 15.750%\n",
      "Training: Epoch 41 :\t Source Accuracy : 99.625%\n",
      "Target Domain Accuracy : 19.867%\n",
      "Training: Epoch 42 :\t Source Accuracy : 99.642%\n",
      "Target Domain Accuracy : 24.133%\n",
      "Training: Epoch 43 :\t Source Accuracy : 99.717%\n",
      "Target Domain Accuracy : 23.892%\n",
      "Training: Epoch 44 :\t Source Accuracy : 99.617%\n",
      "Target Domain Accuracy : 21.425%\n",
      "Training: Epoch 45 :\t Source Accuracy : 99.742%\n",
      "Target Domain Accuracy : 33.133%\n",
      "Training: Epoch 46 :\t Source Accuracy : 99.808%\n",
      "Target Domain Accuracy : 23.300%\n",
      "Training: Epoch 47 :\t Source Accuracy : 99.842%\n",
      "Target Domain Accuracy : 33.008%\n",
      "Training: Epoch 48 :\t Source Accuracy : 99.683%\n",
      "Target Domain Accuracy : 28.208%\n",
      "Training: Epoch 49 :\t Source Accuracy : 99.808%\n",
      "Target Domain Accuracy : 36.433%\n",
      "Training: Epoch 50 :\t Source Accuracy : 99.750%\n",
      "Target Domain Accuracy : 31.875%\n",
      "Training: Epoch 51 :\t Source Accuracy : 99.808%\n",
      "Target Domain Accuracy : 31.983%\n",
      "Training: Epoch 52 :\t Source Accuracy : 99.833%\n",
      "Target Domain Accuracy : 34.392%\n",
      "Training: Epoch 53 :\t Source Accuracy : 99.800%\n",
      "Target Domain Accuracy : 42.683%\n",
      "Training: Epoch 54 :\t Source Accuracy : 99.883%\n",
      "Target Domain Accuracy : 28.808%\n",
      "Training: Epoch 55 :\t Source Accuracy : 99.792%\n",
      "Target Domain Accuracy : 28.308%\n",
      "Training: Epoch 56 :\t Source Accuracy : 99.825%\n",
      "Target Domain Accuracy : 27.608%\n",
      "Training: Epoch 57 :\t Source Accuracy : 99.750%\n",
      "Target Domain Accuracy : 28.258%\n",
      "Training: Epoch 58 :\t Source Accuracy : 99.750%\n",
      "Target Domain Accuracy : 30.408%\n",
      "Training: Epoch 59 :\t Source Accuracy : 99.775%\n",
      "Target Domain Accuracy : 35.917%\n",
      "Training: Epoch 60 :\t Source Accuracy : 99.767%\n",
      "Target Domain Accuracy : 35.408%\n",
      "Training: Epoch 61 :\t Source Accuracy : 99.767%\n",
      "Target Domain Accuracy : 40.650%\n",
      "Training: Epoch 62 :\t Source Accuracy : 99.817%\n",
      "Target Domain Accuracy : 42.275%\n",
      "Training: Epoch 63 :\t Source Accuracy : 99.800%\n",
      "Target Domain Accuracy : 42.217%\n",
      "Training: Epoch 64 :\t Source Accuracy : 99.792%\n",
      "Target Domain Accuracy : 44.408%\n",
      "Training: Epoch 65 :\t Source Accuracy : 99.817%\n",
      "Target Domain Accuracy : 42.017%\n",
      "Training: Epoch 66 :\t Source Accuracy : 99.808%\n",
      "Target Domain Accuracy : 42.233%\n",
      "Training: Epoch 67 :\t Source Accuracy : 99.783%\n",
      "Target Domain Accuracy : 43.925%\n",
      "Training: Epoch 68 :\t Source Accuracy : 99.775%\n",
      "Target Domain Accuracy : 45.650%\n",
      "Training: Epoch 69 :\t Source Accuracy : 99.808%\n",
      "Target Domain Accuracy : 47.958%\n",
      "Training: Epoch 70 :\t Source Accuracy : 99.825%\n",
      "Target Domain Accuracy : 44.483%\n",
      "Training: Epoch 71 :\t Source Accuracy : 99.817%\n",
      "Target Domain Accuracy : 40.392%\n",
      "Training: Epoch 72 :\t Source Accuracy : 99.842%\n",
      "Target Domain Accuracy : 44.683%\n",
      "Training: Epoch 73 :\t Source Accuracy : 99.800%\n",
      "Target Domain Accuracy : 45.658%\n",
      "Training: Epoch 74 :\t Source Accuracy : 99.883%\n",
      "Target Domain Accuracy : 50.633%\n",
      "Training: Epoch 75 :\t Source Accuracy : 99.808%\n",
      "Target Domain Accuracy : 47.942%\n",
      "Training: Epoch 76 :\t Source Accuracy : 99.858%\n",
      "Target Domain Accuracy : 49.542%\n",
      "Training: Epoch 77 :\t Source Accuracy : 99.842%\n",
      "Target Domain Accuracy : 51.908%\n",
      "Training: Epoch 78 :\t Source Accuracy : 99.825%\n",
      "Target Domain Accuracy : 51.492%\n",
      "Training: Epoch 79 :\t Source Accuracy : 99.808%\n",
      "Target Domain Accuracy : 45.950%\n",
      "Training: Epoch 80 :\t Source Accuracy : 99.833%\n",
      "Target Domain Accuracy : 50.717%\n",
      "Training: Epoch 81 :\t Source Accuracy : 99.833%\n",
      "Target Domain Accuracy : 54.725%\n",
      "Training: Epoch 82 :\t Source Accuracy : 99.808%\n",
      "Target Domain Accuracy : 52.850%\n",
      "Training: Epoch 83 :\t Source Accuracy : 99.900%\n",
      "Target Domain Accuracy : 51.458%\n",
      "Training: Epoch 84 :\t Source Accuracy : 99.850%\n",
      "Target Domain Accuracy : 36.117%\n",
      "Training: Epoch 85 :\t Source Accuracy : 99.767%\n",
      "Target Domain Accuracy : 44.050%\n",
      "Training: Epoch 86 :\t Source Accuracy : 99.800%\n",
      "Target Domain Accuracy : 51.242%\n",
      "Training: Epoch 87 :\t Source Accuracy : 99.817%\n",
      "Target Domain Accuracy : 48.417%\n",
      "Training: Epoch 88 :\t Source Accuracy : 99.842%\n",
      "Target Domain Accuracy : 39.883%\n",
      "Training: Epoch 89 :\t Source Accuracy : 99.883%\n",
      "Target Domain Accuracy : 35.825%\n",
      "Training: Epoch 90 :\t Source Accuracy : 99.833%\n",
      "Target Domain Accuracy : 41.217%\n",
      "Training: Epoch 91 :\t Source Accuracy : 99.842%\n",
      "Target Domain Accuracy : 39.183%\n",
      "Training: Epoch 92 :\t Source Accuracy : 99.825%\n",
      "Target Domain Accuracy : 48.358%\n",
      "Training: Epoch 93 :\t Source Accuracy : 99.850%\n",
      "Target Domain Accuracy : 34.192%\n",
      "Training: Epoch 94 :\t Source Accuracy : 99.875%\n",
      "Target Domain Accuracy : 40.633%\n",
      "Training: Epoch 95 :\t Source Accuracy : 99.867%\n",
      "Target Domain Accuracy : 36.742%\n",
      "Training: Epoch 96 :\t Source Accuracy : 99.883%\n",
      "Target Domain Accuracy : 42.292%\n",
      "Training: Epoch 97 :\t Source Accuracy : 99.842%\n",
      "Target Domain Accuracy : 44.000%\n",
      "Training: Epoch 98 :\t Source Accuracy : 99.867%\n",
      "Target Domain Accuracy : 36.858%\n",
      "Training: Epoch 99 :\t Source Accuracy : 99.883%\n",
      "Target Domain Accuracy : 41.725%\n"
     ]
    }
   ],
   "source": [
    "train('domain-adaptation', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbf99d92310>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(da_acc, label='domain_adaptation')\n",
    "plt.plot(test2_acc, label = 'target domain (Real Images)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
